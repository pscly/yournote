## [2026-01-23 21:53] - 任务：发布日记后台异步执行（前端刷新不影响）
- 原始需求：前端点击发布后，即使关闭页面/刷新，后台仍继续发布；前端仅负责触发与查看进度/结果。
- 实现目标/逻辑：
  - 后端新增“启动发布 Run”的接口：收到请求后在后台创建异步任务，按 run 的目标账号列表逐个发布并更新 `publish_diary_run_items.status`。
  - 任务幂等与防重：同一个 run 同时只能有一个后台任务运行；重复 start 请求应返回“已在运行”而不是启动第二份。
  - 进度可见：发布中将 item.status 写为 `running`，成功/失败写入 `success/failed`，错误信息落库，前端轮询即可看到进度变化。
- 核心变更：预计涉及 `backend/app/api/publish_diary.py`、`backend/app/services/background.py`、`backend/app/schemas/publish_diary.py`（状态枚举扩展）。
- 遗留/约束：保持现有表结构不做破坏性改动；尽量避免对 nideriji 的并发压力（后端并发需限制）。

## [2026-01-23 22:04] - 进展：已实现 run 后台发布与防重
- 原始需求：同上。
- 实现目标/逻辑：
  - 新增 `POST /publish-diaries/runs/{run_id}/start`：触发后台异步任务；同一 run 若已在运行则返回 `already_running=true` 防止重复启动。
  - 后台任务会把 `PublishDiaryRunItem.status` 置为 `running`（先提交），然后调用发布接口并落库 `success/failed` 与错误信息，前端轮询即可看到进度变化。
  - 并发限制：通过 `concurrency` 参数 + `asyncio.Semaphore` 控制并发（上限 10）。
- 核心变更：
  - `backend/app/services/background.py`：新增后台发布任务调度与执行（进程内任务表 `_publish_run_tasks`）。
  - `backend/app/api/publish_diary.py`：新增 start 接口并调用 `schedule_publish_run`。
  - `backend/app/schemas/publish_diary.py`：发布条目状态枚举扩展为包含 `running`，并新增 `PublishDiaryStartRunRequest`。
- 遗留/约束：
  - 后台任务为“进程内异步任务”，进程重启不会自动恢复；如需跨重启恢复，可后续增加“扫描未完成 run 并手动恢复/重试”能力。

## [2026-01-23 22:28] - 进展：修复 schemas 导出并完成启动级校验
- 原始需求：继续验证“后台发布不阻塞 + 可刷新继续”的链路，排查潜在运行期问题。
- 实现目标/逻辑：将 `PublishDiaryStartRunRequest` 纳入 `app.schemas` 的统一导出，避免运行期 ImportError；并用真实 import 验证启动链路。
- 核心变更：`backend/app/schemas/__init__.py`（补充 import 与 `__all__`）。
- 遗留/约束：可选优化点：当 force 重试导致状态从 success -> failed 时，是否需要清理旧的 `nideriji_diary_id/response_json` 以避免 UI 误读。

## [2026-01-25 11:12] - 任务：一次性同步本地 SQLite 到远程 PostgreSQL
- 原始需求：部署到服务器，远程用 PostgreSQL；本地继续 SQLite；需要把现有数据同步到远程一次。
- 实现目标/逻辑：
  - 使用现有 `backend/migrate_sqlite_to_postgres.py` 做一次性迁移：源库指向当前开发用的 SQLite（默认 `backend/yournote.db`），目标库通过 `DATABASE_URL` 指向远程 Postgres。
  - 迁移流程：目标库 `create_all` 建表 →（可选）清空目标业务表 → 按依赖顺序逐表拷贝 → 修复自增序列。
  - 安全：连接串密码不写入仓库；执行时通过环境变量或本机 `.env` 临时切换。
- 核心变更：待定（若调整脚本默认源库探测逻辑，将修改 `backend/migrate_sqlite_to_postgres.py`）。
- 遗留/约束：迁移前建议停止正在运行的后端，避免 SQLite 锁；目标库需已创建并保证用户有建表/写入权限。

## [2026-01-25 11:19] - 进展：已完成迁移并核对行数
- 原始需求：同上。
- 实现目标/逻辑：
  - 使用 `backend/migrate_db.py` 从 `backend/yournote.db` 全量迁移到远程 PostgreSQL（通过 `TARGET_DATABASE_URL`），并使用 `--reset-target` 先清空目标库以避免主键冲突。
  - 迁移后对关键表做 COUNT(*) 对账，源/目标行数一致。
  - 安全加固：`backend/migrate_db.py` 增加 `hide_parameters=True` 并在异常时仅输出 `orig`，避免报错把 token/密码等参数打印到控制台。
- 核心变更：`backend/migrate_db.py`（异常脱敏/隐藏参数）。
- 遗留/约束：若远程库需要保留已有数据，请不要使用 `--reset-target`，改为定制 upsert/合并策略。

## [2026-01-25 11:21] - 任务：Docker 化后端（FastAPI + SQLite 默认）
- 原始需求：为项目补齐 Dockerfile + docker-compose；不要引入 PostgreSQL 镜像；让服务器远端可直接部署与更新。
- 实现目标/逻辑：
  - 提供 `backend/Dockerfile`：基于 Python 3.13 + `uv.lock` 构建后端镜像，默认以 `python backend/run.py` 启动，并在 compose 中关闭 `BACKEND_RELOAD`。
  - 数据持久化：通过 volume 把 `./data` 挂载到容器 `/app/data`，并在 compose 中设置 `SQLITE_DB_PATH=./data/yournote.db`；同时挂载 `./logs` 以保留访问日志。
- 核心变更：`backend/Dockerfile`、`docker-compose.yml`（backend service、数据卷与关键环境变量）。
- 遗留/约束：如需切换到远程 PostgreSQL，仅配置 `DATABASE_URL`（本次不提供 pg 镜像）。

## [2026-02-04 23:50] - 任务：记录图片缓存（解析 `[图13]` 并返回稳定图片 URL）
- 原始需求：nideriji 的图片 URL 在浏览器展示为 `blob:`（不可复用）；希望后端缓存图片到本地（优先存数据库），前端查看记录时可直接访问本项目图片并展示附件。
- 实现目标/逻辑：
  - 新增图片缓存表 `cached_images`：按 `(nideriji_userid, image_id)` 唯一定位；存储 `content_type/data/sha256/fetch_status/fetched_at` 等元数据。
  - 新增图片接口 `GET /api/diaries/{diary_id}/images/{image_id}`：优先读缓存；缺失则用账号 `auth_token` 访问上游 `https://f.nideriji.cn/api/image/{userid}/{image_id}/` 拉取并缓存；支持 ETag/304 减少重复传输。
  - 记录详情返回 `attachments.images`：供前端把正文 `[图13]` 原位渲染为 `<img>`。
  - 同步后后台预拉取：仅对本次新增/更新且正文含图片占位符的记录触发；失败不影响同步成功。
- 核心变更：
  - `backend/app/models/cached_image.py`
  - `backend/app/services/image_cache.py`
  - `backend/app/api/diaries.py`（DiaryDetailResponse + attachments + images route）
  - `backend/app/services/collector.py`（sync 成功后 schedule 预拉取）
  - `backend/app/config.py`、仓库根 `.env.example`（新增图片缓存配置项）
- 遗留/约束：
  - 图片接口依赖 Cookie 门禁的同源访问；跨域需额外处理 cookie/cors。

## [2026-02-05 11:20] - 任务：新增配对记录统计支持“按天”（since/until 区间）
- 原始需求：前端希望查看“昨天/前天/某天”的新增配对记录明细，而当前接口只有 since_ms（无终点）。
- 实现目标/逻辑：
  - 为 `/stats/paired-diaries/increase` 增加可选参数 `until_ms`（UTC 毫秒时间戳）。
  - 过滤口径：`created_at >= since_dt AND created_at < until_dt`（左闭右开），以支持“按天”窗口的精确统计。
  - SQLite 兼容：延续现有做法，把 tz-aware 的 UTC datetime 转成 naive 再比较，避免 sqlite datetime 格式差异导致漏算/错算。
- 核心变更（计划）：`backend/app/api/stats.py`。
- 遗留/约束：保持对旧调用方兼容（不传 until_ms 时行为不变）。

## [2026-02-18 21:16] - 任务：新增 msg_count 增量事件表（统计用）
- 原始需求：新增“msg_count 增量事件表”SQLAlchemy 模型并注册到 models；启动时 `init_db()` 的 `Base.metadata.create_all()` 自动建表；字段/索引支持按 `recorded_at` 聚合、按 `(account_id, diary_id)` 分组汇总 delta。
- 实现目标/逻辑：
  - 新增事件表 `diary_msg_count_events`：存储 old/new/delta + recorded_at + source，并通过 FK 关联 account/diary（可选关联 sync_log）。
  - 索引策略：recorded_at 单列索引 + (account_id, diary_id, recorded_at) 复合索引，满足“时间窗口聚合 + 账号隔离分组”的常用查询。
  - 注册方式：在 `backend/app/models/__init__.py` 导入新模型，确保 `from . import models` 会把表注册进 `Base.metadata`。
- 核心变更：
  - `backend/app/models/diary_msg_count_event.py`
  - `backend/app/models/__init__.py`
- 遗留/约束：不引入 Alembic；仅新增表，不修改现有业务表结构；事件写入层面仍需业务代码保证只记录 delta>0。

## [2026-02-19 10:07] - 任务：同步写入 msg_count 增量事件 + stats 聚合接口 + unittest 回归
- 原始需求：显示并统计同步返回的 `diaries.msg_count`；当留言数增加时落库增量事件；提供“全库留言总数/窗口内增量合计与明细”的 stats 接口；补齐最小自动化回归。
- 实现目标/逻辑：
  - 同步：upsert 路径也更新已存在 Diary 的 `msg_count`；当 `msg_count` 增加时写入 `DiaryMsgCountEvent`（只记录 delta>0，按账号隔离）。
  - 统计：`/stats/overview` 返回 `total_msg_count=sum(Diary.msg_count)`；`/stats/msg-count/increase` 按时间窗口聚合 `sum(delta)` 并返回按 `(account_id, diary_id)` 分组的明细。
  - SQLite 兼容：datetime 比较沿用“tz-aware 转 naive UTC”策略，避免 sqlite datetime 格式差异导致漏算。
  - 测试：新增 unittest 覆盖 `total_delta` 不受 limit 影响、分组不串账号、排序与截断等关键口径。
- 核心变更：`backend/app/services/collector.py`、`backend/app/api/stats.py`、`backend/app/schemas/stats.py`、`backend/app/models/diary_msg_count_event.py`、`backend/tests/test_msg_count_increase.py`。
- 遗留/约束：不引入 Alembic；新表通过 `Base.metadata.create_all()` 创建。

## [2026-02-20 14:40] - 任务：新增 diaries.bookmarked_at（schema 自修复 + 索引）
- 原始需求：后端新增 `diaries.bookmarked_at`（epoch ms，可空），旧库启动时可自动补列，并为书签页按收藏时间倒序建立索引。
- 实现目标/逻辑：
  - ORM：`backend/app/models/diary.py` 新增 `bookmarked_at: BigInteger | NULL`（业务语义为 epoch ms）。
  - 自修复：`backend/app/database.py::_ensure_schema()` 在 SQLite 侧用 `PRAGMA table_info(diaries)` 判断缺列后 `ALTER TABLE ... ADD COLUMN bookmarked_at INTEGER`；Postgres 侧使用 `ADD COLUMN IF NOT EXISTS ... BIGINT`。
  - 索引：创建 `idx_diaries_bookmarked_at_desc ON diaries (bookmarked_at DESC)`，以支持按收藏时间排序。
  - 幂等：重复执行 `_ensure_schema()` 不应报错。
- 核心变更：`backend/app/models/diary.py`、`backend/app/database.py`、`backend/tests/test_db_schema_bookmarked_at.py`。
- 遗留/约束：`_ensure_schema()` 会无条件创建多个表索引，做 schema 单测时需要先创建相关最小表结构（如 sync_logs/paired_relationships）。

## [2026-02-20 14:57] - 任务：对外 schema 暴露 bookmarked_at（列表项 + 详情）
- 原始需求：后端对外 schema 暴露 `bookmarked_at`（列表项 + 详情），字段语义为 epoch ms（int，可为 null），且只改 schema 不改查询/组装逻辑。
- 实现目标/逻辑：
  - `DiaryResponse` 新增可空字段 `bookmarked_at`；`DiaryDetailResponse` 继承 `DiaryResponse` 因此自动包含该字段。
  - `DiaryListItemResponse` 新增可空字段 `bookmarked_at`，与 ORM 的 BigInteger（epoch ms）一致。
  - 保持 `Config.from_attributes = True` 不变，避免影响现有 from_attributes 行为。
- 核心变更：`backend/app/schemas/diary.py`、`backend/app/schemas/diary_query.py`。
- 遗留/约束：实际填充/排序逻辑在后续任务处理；若未来路由启用 `response_model_exclude_none/exclude_unset`，null 值可能不会出现在 JSON。

## [2026-02-20 15:10] - 任务：书签 API（幂等 set/clear + 批量取消）+ query 支持书签过滤/排序（含单测）
- 原始需求：为 `diaries.bookmarked_at` 提供幂等收藏写入接口（单条 set/clear + batch set/clear），并让 `GET /api/diaries/query` 支持 `bookmarked` 过滤与 `order_by=bookmarked_at` 的 NULL-last 排序；补齐 unittest 与证据落盘。
- 实现目标/逻辑：
  - 幂等写入：使用条件 UPDATE（set: `WHERE bookmarked_at IS NULL` / clear: `WHERE bookmarked_at IS NOT NULL`），再 SELECT 返回最终 `bookmarked_at`；缺失 diary 返回 404。
  - 查询排序：`order_by=bookmarked_at` 先按 `bookmarked_at IS NULL` 升序实现 NULL-last，再按主排序 + `created_date + id` 做稳定 tie-breaker。
- 核心变更：
  - 路由/Schema 已在 `backend/app/api/diaries.py`、`backend/app/schemas/diary_bookmark.py`、`backend/app/schemas/__init__.py` 完整落地；本次补齐 `backend/tests/test_diary_bookmarks.py` 的 batch clear 与 asc/desc 排序回归，并修正基于 basedpyright 的异常类型引用。
- 验证：
  - `uv run python -m unittest -v backend/tests/test_diary_bookmarks.py` 通过；输出与 404 证据写入 `.sisyphus/evidence/`。

## [2026-02-20 23:30] - 任务：diaries/query 支持 has_msg 过滤 + msg_count 排序
- 原始需求：前端要新增“留言记录”页面，需要后端查询接口能过滤“仅有留言（msg_count>0）”并支持按留言数排序。
- 实现目标/逻辑：
  - `GET /api/diaries/query` 增加参数 `has_msg`：
    - `true`：仅 `msg_count>0`
    - `false`：仅 `msg_count<=0`
    - `null`：不过滤
  - `order_by` 扩展为支持 `msg_count`，并与既有稳定排序（created_date/id）兼容。
- 核心变更：
  - `backend/app/api/diaries.py`：query 参数校验、where 条件、排序字段映射。
  - 新增 `backend/tests/test_diary_query_has_msg.py`：覆盖 `has_msg` 过滤与 `order_by=msg_count` 排序。
- 验证：
  - `uv run python -m unittest -v backend/tests/test_diary_query_has_msg.py` 通过。
  - `uv run python -m unittest -v backend/tests/test_postgres_distinct_orderby_regression.py` 回归通过。
- 遗留/约束：无（未引入新依赖；未做 schema/索引迁移）。
